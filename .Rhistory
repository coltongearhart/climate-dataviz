#| label: check_data_2004
# confirm this by counting the number of observations per day for each element / location combo
# -> filter to only 2004 data
# -> split into dataframe subsets for each element
# -> do the counting on each subset
# -> combine results into an organized dataframe
(data_2004_check <- data_climate %>%
filter(year(Date) == 2004) %>%
split(.$Element) %>%
mclapply(FUN = function(element_data) {
# count the number of rows per day
# add back in the element info for when combining the results later
element_data %>%
count(Location, Date) %>%
mutate(Element = unique(element_data$Element))
}, mc.cores = 4) %>%
Reduce(f = bind_rows, x = .))
# Chunk 10: adjust_data_2004
#| label: adjust_data_2004
# adjust data from 2004
# -> filter climate data to only 2004
# -> group by identifying variables
# -> average the multiple observations
(data_2004 <- data_climate %>%
filter(year(Date) == 2004) %>%
group_by(Element, Location, Date) %>%
summarize(Value = mean(Value)) %>%
ungroup)
# Chunk 11: recheck_data_2004
#| label: recheck_data_2004
# now run the summarized data back through the checking procedure to confirm that there is now only one obs per day
(data_2004_recheck <- data_2004 %>%
split(.$Element) %>%
mclapply(FUN = function(element_data) {
element_data %>%
count(Location, Date) %>%
mutate(Element = unique(element_data$Element))
}, mc.cores = 4) %>%
Reduce(f = bind_rows, x = .))
# Chunk 12: more-data-manipulations
#| label: more-data-manipulations
# now continue with refining climate dataset
# -> remove original 2004 data
# -> add in the corrected 2004 data to the rest of the correct data
# -> sort in a logical way
# -> adjust values to have readable units (originally in 10ths of degrees and 10ths of mm)
# -> remove original value variable
# this is a final plotting dataset as well as the base dataset that future datasets start with
(data_climate %<>%
filter(year(Date) != 2004) %>%
bind_rows(data_2004) %>%
arrange(Element, Location, Date) %>%
mutate(value = Value / 10) %>%
select(-Value))
# Chunk 13: check-final-data
#| label: check-final-data
# check years for final dataset by running the data through the counting and plotting process from above
data_climate %>%
count(Location,
year = year(Date),
Element) %>%
{ggplot() +
geom_point(aes(x = year,
y = n),
data = .) +
geom_hline(yintercept = 365,
color = "blue") +
scale_y_continuous(limits = c(0,400)) +
facet_grid(Location ~ Element)}
# Chunk 14: data-monthly-average
#| label: data-monthly-average
# create dataset that contains average values for each month by location
# this is a final plotting dataset
# -> extract the year and month of the date for grouping purposes
# -> set day to be the last day of each month and average the values for each month
# -> combine individual date related variables back into a full date variable and create a grouping variable (which is just for the line plot) that defines whether a time point is before or after the missing year (2003) of data
# -> select only the now wanted variables
# --> had to ungroup in prior step so that year and month aren't automatically included in the select statement by dplyr
(data_monlthy_avg <- data_climate %>%
mutate(year = year(Date),
month = month(Date)) %>%
group_by(Location,
year,
month,
Element) %>%
summarize(day = max(day(Date)),
value_avg = round(mean(value), 1)) %>%
ungroup %>%
mutate(date = ymd(paste(year, month, day, sep = "-")),
line_plot_group = ifelse(test = year < 2003,
yes = 1,
no = 2)) %>%
select(Location,
date,
Element,
value_avg,
line_plot_group))
# Chunk 15: data-projections
#| label: data-projections
# create a dataset that will be used to plot a horizontal line
# this is an intermediate dataset for the final plotting data
# the y value should equal the average value during first five years of data collection
# and the x values will be the first and last dates of data collection so that the line spans the entire time frame
# -> group by element and location (so the next calculations are group-wise)
# -> calculate year, the minimum year (in order to get the limit year for the 5 year avg) and the exact first and last dates of data collection
# -> filter data to only observations in the first 5 years of data collection
# -> group by again, we need to group by first and last date as well in order to keep those columns (it doesn't affect the following summarizing calculations)
# -> calculate the average temps for each location
(data_baseline <- data_climate %>%
group_by(Element,
Location) %>%
mutate(year = year(Date),
year_first = min(year),
date_first = min(Date),
date_last = max(Date)) %>%
filter(year < year_first + 5) %>%
group_by(Element,
Location,
date_first,
date_last) %>%
summarize(value_5avg = round(mean(value), 1)) %>%
ungroup)
# Chunk 16: data-late-pred
#| label: data-late-pred
# goal is to get the predicted value from the last day of data collection based off some model
# going to do loess regression with response as value (value) and predictor as number of days after (days_since) a round arbitrary start date (01/01/1950) that is before earliest data point
# this is an intermediate dataset for the final plotting data
# -> calculate explanatory variable
# -> split dataset into a list with an element for each unique Location / Element combo
# -> run lapply for each element that does the following:
# --> fits a model and all extract predictions
# --> then create and return a dataframe that contains only the final predicted value and the location / element information
# -> finally combine each returned one-row dataframes into a single dataframe
(data_last_pred <- data_climate %>%
mutate(days_since = as.numeric(Date - ymd("1950-01-01"))) %>%
split(f = paste(.$Location,
.$Element,
sep = "-")) %>%
mclapply(FUN = function(climate_data) {
# fit loess model with specified response and predictor
# -> the model fit here matches what will be done with geom_smooth() in the final plot
# -> so using the same model parameters, which is span = 2 and is largerish more robust so that the smoothed line is not too fine / jaggedy (want it to be a smooth curve for visual purposes)
model = loess(formula = value ~ days_since,
span = 2,
data = climate_data)
# extract predicted values across all xs
preds = predict(model) %>%
round(2)
# create an organized one-row organized dataframe to return
# -> reduce modeling dataset to just one row that contains that needed id information
# -> extract just the last predicted value
climate_data %>%
distinct(Location,
Element) %>%
mutate(last_pred = tail(preds, n = 1)) %>%
return
}, mc.cores = 4) %>%
Reduce(f = bind_rows, x = .))
# Chunk 17: data-projections-final
#| label: data-projections-final
# create dataset that contains all of the information related to projections, both the simple baseline average and the fancy model
# this is a final plotting dataset
# -> add info about the final predicted value to the baseline data
# now need to create the information about the error bar, which will summarize the differences between the two different projection methods
# -> do the following calculations:
# --> difference between the two final predictions
# --> sign of difference (for the label)
# --> character string of formatted error label
# --> location of midpoint of errorbar, which will be the y value of the label position on the final plot
(data_projections <- data_baseline %>%
left_join(data_last_pred, by = c("Element", "Location")) %>%
ungroup %>%
mutate(error = round(last_pred - value_5avg, 2),
error_sign = ifelse(error > 0, "+", "-"),
error_label = paste(ifelse(error_sign == "+", "+", ""),
error,
"˚",
sep = ""),
error_midpoint = round((last_pred + value_5avg) / 2, 2)))
# Chunk 18: data-windows
#| label: data-windows
# create dataset with the information needed for y limits of each panel
# this is a final plotting dataset
# -> start with the monthly average values dataset because that data defines the y scale
# -> group by element and location
# -> figure out the min, max and range (max - min) values for each
# now find the max range for each element
# summarizing over Location because we want to the overall max as the new common max
# do this by splitting the dataframe by element and an lapply that does the following on each subset:
# -> figures out the max range
# -> adds this info to the original subset with a join
# -> then combine resulting dataframes back into one overall dataframe
# now we are going to add in the information about the baseline average
# -> use a join, just want the first date and 5 year avg info from the baseline info
# -> select and reorder info so can figure out how to set limits
# -> add width info to baseline dataset
# manually set bounds for each panels window (just looked at numbers and set window bounds strategically)
# did this according to the following strategy (for each element):
# -> window for each panel (location) needs to be the same as the overall max range so that can compare error bars accurately
# -> and preferably centered around the 5 year average, just for aesthetic purposes
# -> add this via numeric constants in a mutate statement
# -> note that rain fall has a natural lower bound of zero, so curves wont really be centered in the windows
# -> and windows for rain are quite large (relative to values for AK and AU) because of the high variability in HI
# now want to format data so that it can be handled by ggplot and the facets
# -> convert to to tall data, creating a column for the value of the bound and which window it corresponds to (lower / upper)
# -> keep only needed variables
# --> going to use the the first day of data collection as the x coordinate, it's just an arbitrary choice
(data_y_limits <- data_monlthy_avg %>%
group_by(Element,
Location) %>%
summarize(across(.cols = value_avg,
.fns = c("min" = min, "max" = max),
.names = "{.fn}"),
range = diff(range(value_avg))) %>%
split(f = .$Element) %>%
mclapply(FUN = function(avg_data) {
# -> calculate the maximum range across locations
# -> then add back in original info
# -> remove individual location range values
avg_data %>%
summarize(max_range = max(range)) %>%
right_join(avg_data,
by = "Element") %>%
select(-range) %>%
return
}, mc.cores = 4) %>%
Reduce(f = bind_rows,
x = .) %>%
right_join(data_baseline,
by = c("Element",
"Location")) %>%
select(Element,
Location,
date_first,
min,
max,
max_range,
value_5avg) %>%
mutate(lower = c(0,0,0,
-13,3.7,10.1,
-20,-5,2.8),
upper = c(35,35,35,
20.8,37.5,43.9,
12,25.8,33.6),
window_capture = lower < min && upper > max) %>%
pivot_longer(cols = c(lower,
upper),
names_to = "y_window",
values_to = "bound") %>%
select(Element,
Location,
date_first,
y_window,
bound))
data_monlthy_avg %>%
group_by(Element,
Location) %>%
summarize(across(.cols = value_avg,
.fns = c("min" = min, "max" = max),
.names = "{.fn}"),
range = diff(range(value_avg))) %>%
split(f = .$Element)
# -> keep only needed variables
# --> going to use the the first day of data collection as the x coordinate, it's just an arbitrary choice
(data_y_limits <- data_monlthy_avg %>%
group_by(Element,
Location) %>%
summarize(across(.cols = value_avg,
.fns = c("min" = min, "max" = max),
.names = "{.fn}"),
range = diff(range(value_avg))) %>%
split(f = .$Element) %>%
mclapply(FUN = function(avg_data) {
# -> calculate the maximum range across locations
# -> then add back in original info
# -> remove individual location range values
avg_data %>%
summarize(max_range = max(range)) %>%
right_join(avg_data,
by = "Element") %>%
select(-range) %>%
return
}, mc.cores = 4) %>%
Reduce(f = bind_rows,
x = .) %>%
right_join(data_baseline,
by = c("Element",
"Location")) %>%
select(Element,
Location,
date_first,
min,
max,
max_range,
value_5avg)
data_y_limits <- data_monlthy_avg %>%
group_by(Element,
Location) %>%
summarize(across(.cols = value_avg,
.fns = c("min" = min, "max" = max),
.names = "{.fn}"),
range = diff(range(value_avg))) %>%
split(f = .$Element) %>%
mclapply(FUN = function(avg_data) {
# -> calculate the maximum range across locations
# -> then add back in original info
# -> remove individual location range values
avg_data %>%
summarize(max_range = max(range)) %>%
right_join(avg_data,
by = "Element") %>%
select(-range) %>%
return
}, mc.cores = 4) %>%
Reduce(f = bind_rows,
x = .) %>%
right_join(data_baseline,
by = c("Element",
"Location")) %>%
select(Element,
Location,
date_first,
min,
max,
max_range,
value_5avg)
View(data_y_limits)
data_y_limits %>%  mutate(lower = c(0,0,0,
-13,3.7,10.1,
-20,-5,2.8),
upper = c(35,35,35,
20.8,37.5,43.9,
12,25.8,33.6),
window_capture = lower < min && upper > max)
data_y_limits %>%  mutate(lower = c(0,0,0,
-13,3.7,10.1,
-20,-5,2.8),
upper = c(35,35,35,
20.8,37.5,43.9,
12,25.8,33.6))
# did this according to the following strategy (for each element):
# -> window for each panel (location) needs to be the same as the overall max range so that can compare error bars accurately
# -> and preferably centered around the 5 year average, just for aesthetic purposes
# -> add this via numeric constants in a mutate statement
# -> note that rain fall has a natural lower bound of zero, so curves wont really be centered in the windows
# -> and windows for rain are quite large (relative to values for AK and AU) because of the high variability in HI
# now want to format data so that it can be handled by ggplot and the facets
# -> convert to to tall data, creating a column for the value of the bound and which window it corresponds to (lower / upper)
# -> keep only needed variables
# --> going to use the the first day of data collection as the x coordinate, it's just an arbitrary choice
(data_y_limits <- data_monlthy_avg %>%
group_by(Element,
Location) %>%
summarize(across(.cols = value_avg,
.fns = c("min" = min, "max" = max),
.names = "{.fn}"),
range = diff(range(value_avg))) %>%
split(f = .$Element) %>%
mclapply(FUN = function(avg_data) {
# -> calculate the maximum range across locations
# -> then add back in original info
# -> remove individual location range values
avg_data %>%
summarize(max_range = max(range)) %>%
right_join(avg_data,
by = "Element") %>%
select(-range) %>%
return
}, mc.cores = 4) %>%
Reduce(f = bind_rows,
x = .) %>%
right_join(data_baseline,
by = c("Element",
"Location")) %>%
select(Element,
Location,
date_first,
min,
max,
max_range,
value_5avg) %>%
mutate(lower = c(0,0,0,
-13,3.7,10.1,
-20,-5,2.8),
upper = c(35,35,35,
20.8,37.5,43.9,
12,25.8,33.6)) %>%
pivot_longer(cols = c(lower,
upper),
names_to = "y_window",
values_to = "bound") %>%
select(Element,
Location,
date_first,
y_window,
bound))
# define a function that will return a ggplot object with all of the above features
# -> only need to specify character strings for the element of interest and the main color of the line plots
# --> default color value is black (which is neutralish) just so that can run code if don't want to specify one
# this will contain only plot feature that are common across all elements
# element specific features (such as labels and scale colors) will be tacked on afterwards, as well as the formal titles / captions
# so when plotting a specific element, just have to call this function and then the rest of the layers that are modified to have info specific to the element of interest
# note that this function depends on objects in the global environment
# -> so not necessarily the best way to code it
# -> but the datasets it requires have to be setup in a specific way according to the above data manipulations
# -> so this is just much easier and makes sense for how this application was carried out
fun_common_plot <- function(element,
color = "black",
window_setting = FALSE) {
# create and save base plot object
g = ggplot() +
geom_line(aes(x = date,
y = value_avg,
group = line_plot_group),
color = color,
size = 0.25,
alpha = 0.25,
data = filter(data_monlthy_avg,
Element == element)) +
stat_smooth(aes(x = Date,
y = value),
method = "loess",
span = 2,
se = FALSE,
color = color,
data = filter(data_climate,
Element == element)) +
geom_segment(aes(x = date_first,
xend = date_last,
y = value_5avg,
yend = value_5avg),
color = "grey50",
data = filter(data_projections,
Element == element)) +
geom_errorbar(aes(x = date_last,
ymin = value_5avg,
ymax = last_pred,
color = error_sign),
width = 500,
size = 1,
show.legend = FALSE,
data = filter(data_projections,
Element == element)) +
geom_text(aes(x = date_last,
y = error_midpoint,
label = error_label,
color = error_sign),
nudge_x = 1500,
size = 5,
show.legend = FALSE,
data = filter(data_projections,
Element == element)) +
facet_grid(Location ~ .,
scales = "free_y") +
theme(axis.title.x = element_blank(),
panel.border = element_rect(color = "lightgrey",
fill = NA))
# conditionally add range setting feature
if (window_setting) {
g = g +
geom_point(aes(x = date_first,
y = bound),
alpha = 0,
data = filter(data_y_limits,
Element == element))
}
# return plot object
return(g)
}
# visualize TMIN
# -> first visualize base plot
fun_common_plot(element = "TMIN",
color = "dodgerblue1",
window_setting = FALSE)
# -> then add the rest of the layers with formal info
(plot_tmin <- fun_common_plot(element = "TMIN",
color = "dodgerblue1",
window_setting = TRUE) +
scale_color_manual(values = c("-" = "grey60", "+" = "orange")) +
labs(title = "Average minimum temperatures over time \ncompared to average during first five years",
subtitle = "(with monthly averages overlaid)",
caption = "Data source: Daily Global Historical Climatology Network (GHCN) \nPlot by Colton Gearhart",
y = "Temperature (˚C)"))
# -> then add the rest of the layers with formal info
# this is the final visualization for maximum temperatures
(plot_tmax <- fun_common_plot(element = "TMAX",
color = "dodgerblue4",
window_setting = TRUE) +
scale_color_manual(values = c("-" = "grey60", "+" = "orange")) +
labs(title = "Average maximum temperatures over time \ncompared to average during first five years",
subtitle = "(with monthly averages overlaid)",
caption = "Data source: Daily Global Historical Climatology Network (GHCN) \nPlot by Colton Gearhart",
y = "Temperature (˚C)"))
# -> then add the rest of the layers with formal info
(plot_prcp <- fun_common_plot(element = "PRCP",
color = "turquoise",
window_setting = TRUE) +
scale_color_manual(values = c("-" = "gold2", "+" = "plum2")) +
labs(title = "Average precipitation levels over time \ncompared to average during first five years",
subtitle = "(with monthly averages overlaid)",
caption = "Data source: Daily Global Historical Climatology Network (GHCN) \nPlot by Colton Gearhart",
y = "Precipitation (mm)"))
# save plots
ggsave(filename = "Files/Images/Visual-TMAX.png",
plot = plot_tmin)
ggsave(filename = "Files/Images/Visual-TMAX.png",
plot = plot_tmax)
ggsave(filename = "Files/Images/Visual-PRCP.png",
plot = plot_prcp)
# save plots
ggsave(filename = "Files/Images/Visual-TMIN.png",
plot = plot_tmin)
