[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Climate Change Data Visualization",
    "section": "",
    "text": "Preface\nThis is a data visualization project on climate changed completed in graduate school.\n\n\nChapter 1 goes through the entire visualization process from data manipulation to creating the final visuals.\nChapter 2 is the final written report.\n\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/climate-dataviz",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "visualizations.html#read-in-data",
    "href": "visualizations.html#read-in-data",
    "title": "1  Visualizations",
    "section": "\n1.2 Read in data",
    "text": "1.2 Read in data\nData is from the Daily Global Historical Climatology Network (GHCN). It only includes three selected stations (one in AK, AU and HI) and ranges from years 1950 to 2019. The units for elements of interest (TMIN & TMAX: tenths of degrees celsius; PRCP: tenths of mm; SNOW & SNWD: mm).\n\nCode# read in data\n(data_climate_raw &lt;- read_csv(\"files/data/climate-data.csv\",\n                         col_names = TRUE, col_types = c(\"ccDcnccc\")))\n\n# A tibble: 268,454 × 8\n   StationID   Location Date       Element Value MFlag QFlag SFlag\n   &lt;chr&gt;       &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 USC00511125 HI       1950-12-01 TMAX      239 &lt;NA&gt;  &lt;NA&gt;  0    \n 2 USC00511125 HI       1950-12-01 TMIN      200 &lt;NA&gt;  &lt;NA&gt;  0    \n 3 USC00511125 HI       1950-12-01 PRCP       79 &lt;NA&gt;  &lt;NA&gt;  0    \n 4 USC00511125 HI       1950-12-01 SNOW        0 &lt;NA&gt;  &lt;NA&gt;  0    \n 5 USC00511125 HI       1950-12-01 SNWD        0 &lt;NA&gt;  &lt;NA&gt;  0    \n 6 USC00511125 HI       1950-12-02 TMAX      233 &lt;NA&gt;  &lt;NA&gt;  0    \n 7 USC00511125 HI       1950-12-02 TMIN      189 &lt;NA&gt;  &lt;NA&gt;  0    \n 8 USC00511125 HI       1950-12-02 PRCP      269 &lt;NA&gt;  &lt;NA&gt;  0    \n 9 USC00511125 HI       1950-12-02 SNOW        0 &lt;NA&gt;  &lt;NA&gt;  0    \n10 USC00511125 HI       1950-12-02 SNWD        0 &lt;NA&gt;  &lt;NA&gt;  0    \n# ℹ 268,444 more rows",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#base-data-manipulation",
    "href": "visualizations.html#base-data-manipulation",
    "title": "1  Visualizations",
    "section": "\n1.3 Base data manipulation",
    "text": "1.3 Base data manipulation\n\n1.3.1 Basic data cleaning\nNow lets do some basic cleaning of the data, including removing unnecessary columns and filtering to only the initial elements of interest.\n\nCode# clean up data\n# -&gt; remove unnecessary columns\n# -&gt; filter to only desired elements\n(data_clean &lt;- data_climate_raw %&gt;% \n  select(-ends_with(\"Flag\"),\n         -StationID) %&gt;% \n  filter(Element %in% c(\"TMIN\", \"TMAX\", \"PRCP\", \"SNOW\", \"SNWD\")))\n\n# A tibble: 236,232 × 4\n   Location Date       Element Value\n   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 HI       1950-12-01 TMAX      239\n 2 HI       1950-12-01 TMIN      200\n 3 HI       1950-12-01 PRCP       79\n 4 HI       1950-12-01 SNOW        0\n 5 HI       1950-12-01 SNWD        0\n 6 HI       1950-12-02 TMAX      233\n 7 HI       1950-12-02 TMIN      189\n 8 HI       1950-12-02 PRCP      269\n 9 HI       1950-12-02 SNOW        0\n10 HI       1950-12-02 SNWD        0\n# ℹ 236,222 more rows\n\n\n\n1.3.2 Missing data\nNext step is to check to see if there is any missing data. To do this, we can count the number of observations for each year in each respective group. We expect there to be one observation per day, so ideally approximately 365 observations per year at each location for each measurement.\n\nCode# check dataset for years with missing data\n# -&gt; count the number of days within each year that have a measurement (for all elements from each location)\n(data_years &lt;- data_clean %&gt;% \n  count(Location,\n        year = year(Date),\n        Element))\n\n# A tibble: 711 × 4\n   Location  year Element     n\n   &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;   &lt;int&gt;\n 1 AK        1963 PRCP       55\n 2 AK        1963 SNOW       55\n 3 AK        1963 SNWD       55\n 4 AK        1963 TMAX       55\n 5 AK        1963 TMIN       55\n 6 AK        1964 PRCP       89\n 7 AK        1964 SNOW       89\n 8 AK        1964 SNWD       14\n 9 AK        1964 TMAX       87\n10 AK        1964 TMIN       87\n# ℹ 701 more rows\n\n\nIn order to find things to look into more, lets create a visual for a quick inspection of the count data.\n\nCode# create plot to visualize the wide data, which represents the total number of days with observations for each year\n# do this for each location and element combination\n# add a horizontal line at 365 days, which contextually represents data collected everyday of said year\nggplot() + \n  geom_point(aes(x = year,\n                 y = n),\n             data = data_years) + \n  geom_hline(yintercept = 365,\n             color = \"blue\") + \n  facet_grid(Location ~ Element)\n\n\n\n\n\n\n\n\n\nHere are some observations:\n\nNo snow data for AU.\nA few years with more 365 obs.\nOnly consistent full years of data for AK after around 1980.\nNot consistent full year of data for HI beyond 2002.\n\nThoughts on these:\n\nShould can not work with SNOW and SNWD moving forward. This is because the final goal is to be able to visualize elements that have data for all three locations.\nNot sure what this means, so we might want to consider removing these data points? Unclear at this time.\nShould only use data for years 1980 and beyond for AK moving forward.\nShould only use data up to and including year 2002 for HI moving forward.\n\nNow that we have visually inspected the count data, we can make some initial decisions about which data to work or not work with. Getting these out of the way before diving in more will make it easier.\n\nCode# start refining the data\n# -&gt; keep only the elements that are going to be visualized\n(data_climate &lt;- data_clean %&gt;%\n  filter(Element %in% c(\"TMIN\",\n                        \"TMAX\",\n                        \"PRCP\")))\n\n# A tibble: 163,669 × 4\n   Location Date       Element Value\n   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 HI       1950-12-01 TMAX      239\n 2 HI       1950-12-01 TMIN      200\n 3 HI       1950-12-01 PRCP       79\n 4 HI       1950-12-02 TMAX      233\n 5 HI       1950-12-02 TMIN      189\n 6 HI       1950-12-02 PRCP      269\n 7 HI       1950-12-03 TMAX      228\n 8 HI       1950-12-03 TMIN      194\n 9 HI       1950-12-03 PRCP      457\n10 HI       1950-12-04 TMAX      267\n# ℹ 163,659 more rows\n\n\nNow lets look into the count data with only the desired elements to see exactly what’s going on.\n\nCode# look further into count data\n# convert to wide data to be able to compactly look at how much data there is (or is not) for each element\n# can also manually sort by year to look at locations within year when inspecting\n(data_years_wide &lt;- data_climate %&gt;% \n  count(Location,\n        year = year(Date),\n        Element) %&gt;% \n  pivot_wider(names_from = \"Element\",\n              values_from = \"n\"))\n\n# A tibble: 163 × 5\n   Location  year  PRCP  TMAX  TMIN\n   &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 AK        1963    55    55    55\n 2 AK        1964    89    87    87\n 3 AK        1965   120   120   120\n 4 AK        1966   119   120   120\n 5 AK        1967   165   202   202\n 6 AK        1968   211   326   326\n 7 AK        1969    55    61    61\n 8 AK        1970   349   356   356\n 9 AK        1971   363   327   328\n10 AK        1972   354   297   297\n# ℹ 153 more rows\n\n\nAfter closely looking through this representation of the count data, we can confirm the several of the observations made from the first visual. Low number of observations lead us to focus on data within locations that do not include the following years of data:\n\nSets of years where there is consistently not good data.\nOne-off years with only a few observations.\n\nAlso note that there is no data in the original dataset from 2003 for some reason. This will obviously show up in the final visuals, but nothing that can be done about it (although measures will be taken to ensure accurate portrayals as described later).\n\nCode# continue refining climate data to move forward with\n# -&gt; remove years with not good data\n(data_climate %&lt;&gt;% \n  filter(!(Location == \"AK\" & (year(Date) &lt; 1980 | year(Date) == 2019)),\n         !(Location == \"AU\" & (year(Date) %in% c(1970, 2019))),\n         !(Location == \"HI\" & (year(Date) &gt; 2002 | year(Date) == 1950))))\n\n# A tibble: 149,826 × 4\n   Location Date       Element Value\n   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 HI       1951-01-01 PRCP        0\n 2 HI       1951-01-02 TMIN      228\n 3 HI       1951-01-02 PRCP        0\n 4 HI       1951-01-03 TMIN      206\n 5 HI       1951-01-03 PRCP       64\n 6 HI       1951-01-04 TMIN      200\n 7 HI       1951-01-04 PRCP       15\n 8 HI       1951-01-05 TMIN      194\n 9 HI       1951-01-05 PRCP        8\n10 HI       1951-01-06 TMIN      194\n# ℹ 149,816 more rows\n\n\n\n1.3.3 Extra observations\nNow lets look into data from 2004. As of now, we are not sure why there are multiple observations for each date across all elements.\n\nCode# confirm this by counting the number of observations per day for each element / location combo\n# -&gt; filter to only 2004 data\n# -&gt; split into dataframe subsets for each element\n# -&gt; do the counting on each subset\n# -&gt; combine results into an organized dataframe\n(data_2004_check &lt;- data_climate %&gt;% \n  filter(year(Date) == 2004) %&gt;% \n  split(.$Element) %&gt;% \n  mclapply(FUN = function(element_data) {\n    \n    # count the number of rows per day\n    # add back in the element info for when combining the results later\n    element_data %&gt;% \n      count(Location, Date) %&gt;% \n      mutate(Element = unique(element_data$Element))\n    \n    }, mc.cores = 4) %&gt;% \n  Reduce(f = bind_rows, x = .))\n\n# A tibble: 2,106 × 4\n   Location Date           n Element\n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;chr&gt;  \n 1 AK       2004-01-01     2 PRCP   \n 2 AK       2004-01-02     2 PRCP   \n 3 AK       2004-01-03     2 PRCP   \n 4 AK       2004-01-04     2 PRCP   \n 5 AK       2004-01-05     2 PRCP   \n 6 AK       2004-01-06     2 PRCP   \n 7 AK       2004-01-07     2 PRCP   \n 8 AK       2004-01-08     2 PRCP   \n 9 AK       2004-01-09     2 PRCP   \n10 AK       2004-01-10     2 PRCP   \n# ℹ 2,096 more rows\n\n\nWe see \\(n = 2\\), so multiple observations for each day is confirmed. To consolidate these, we are going to average all observations from a single day. This is to have just a single value per day, which will ensure that each day has the same weight when taking monthly averages later. If one day has more than one record, then it would be like there is an extra day, which is an inaccurate interpretation.\n\nCode# adjust data from 2004\n# -&gt; filter climate data to only 2004\n# -&gt; group by identifying variables\n# -&gt; average the multiple observations\n(data_2004 &lt;- data_climate %&gt;% \n  filter(year(Date) == 2004) %&gt;% \n  group_by(Element, Location, Date) %&gt;% \n  summarize(Value = mean(Value)) %&gt;% \n  ungroup)\n\n# A tibble: 2,106 × 4\n   Element Location Date       Value\n   &lt;chr&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;dbl&gt;\n 1 PRCP    AK       2004-01-01     0\n 2 PRCP    AK       2004-01-02     0\n 3 PRCP    AK       2004-01-03     0\n 4 PRCP    AK       2004-01-04     0\n 5 PRCP    AK       2004-01-05     0\n 6 PRCP    AK       2004-01-06     0\n 7 PRCP    AK       2004-01-07     0\n 8 PRCP    AK       2004-01-08     0\n 9 PRCP    AK       2004-01-09   185\n10 PRCP    AK       2004-01-10   231\n# ℹ 2,096 more rows\n\n\nNow lets count the adjusted data to confirm that is has been correctly addressed.\n\nCode# now run the summarized data back through the checking procedure to confirm that there is now only one obs per day\n(data_2004_recheck &lt;- data_2004 %&gt;% \n  split(.$Element) %&gt;% \n  mclapply(FUN = function(element_data) {\n    \n    element_data %&gt;% \n      count(Location, Date) %&gt;% \n      mutate(Element = unique(element_data$Element))\n    \n  }, mc.cores = 4) %&gt;% \n  Reduce(f = bind_rows, x = .))\n\n# A tibble: 2,106 × 4\n   Location Date           n Element\n   &lt;chr&gt;    &lt;date&gt;     &lt;int&gt; &lt;chr&gt;  \n 1 AK       2004-01-01     1 PRCP   \n 2 AK       2004-01-02     1 PRCP   \n 3 AK       2004-01-03     1 PRCP   \n 4 AK       2004-01-04     1 PRCP   \n 5 AK       2004-01-05     1 PRCP   \n 6 AK       2004-01-06     1 PRCP   \n 7 AK       2004-01-07     1 PRCP   \n 8 AK       2004-01-08     1 PRCP   \n 9 AK       2004-01-09     1 PRCP   \n10 AK       2004-01-10     1 PRCP   \n# ℹ 2,096 more rows\n\n\n\n1.3.4 Final modifications\nNow can make the final refinements to the climate dataset.\n\nCode# now continue with refining climate dataset\n# -&gt; remove original 2004 data\n# -&gt; add in the corrected 2004 data to the rest of the correct data\n# -&gt; sort in a logical way\n# -&gt; adjust values to have readable units (originally in 10ths of degrees and 10ths of mm)\n# -&gt; remove original value variable\n# this is a final plotting dataset as well as the base dataset that future datasets start with \n(data_climate %&lt;&gt;% \n  filter(year(Date) != 2004) %&gt;% \n  bind_rows(data_2004) %&gt;% \n  arrange(Element, Location, Date) %&gt;% \n  mutate(value = Value / 10) %&gt;% \n  select(-Value))\n\n# A tibble: 147,720 × 4\n   Location Date       Element value\n   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;   &lt;dbl&gt;\n 1 AK       1980-01-01 PRCP      0  \n 2 AK       1980-01-02 PRCP      0  \n 3 AK       1980-01-03 PRCP      0  \n 4 AK       1980-01-04 PRCP      0  \n 5 AK       1980-01-05 PRCP      8.6\n 6 AK       1980-01-06 PRCP      0  \n 7 AK       1980-01-07 PRCP      0  \n 8 AK       1980-01-08 PRCP      0  \n 9 AK       1980-01-09 PRCP      0  \n10 AK       1980-01-10 PRCP      0  \n# ℹ 147,710 more rows\n\n\nAs a final confirmation, lets recreate the first visual with the new refined dataset.\n\nCode# check years for final dataset by running the data through the counting and plotting process from above\ndata_climate %&gt;% \n  count(Location,\n        year = year(Date),\n        Element) %&gt;% \n  {ggplot() + \n      geom_point(aes(x = year,\n                     y = n),\n                 data = .) + \n      geom_hline(yintercept = 365,\n                 color = \"blue\") + \n      scale_y_continuous(limits = c(0,400)) + \n      facet_grid(Location ~ Element)}\n\n\n\n\n\n\n\n\n\nSo now we have a dataset that contains data only from the elements of interest and the valid years for each respective location and correct units. It is now ready to be modified as needed for the desired visuals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#create-visualization-data",
    "href": "visualizations.html#create-visualization-data",
    "title": "1  Visualizations",
    "section": "\n1.4 Create visualization data",
    "text": "1.4 Create visualization data\nWith an idea of the final visuals in mine, we can start to construct the needed data for each feature of the plot.\n\n1.4.1 Monthly averages\nFirst is the monthly averages for each month within each location.\n\nCode# create dataset that contains average values for each month by location\n# this is a final plotting dataset\n# -&gt; extract the year and month of the date for grouping purposes \n# -&gt; set day to be the last day of each month and average the values for each month\n# -&gt; combine individual date related variables back into a full date variable and create a grouping variable (which is just for the line plot) that defines whether a time point is before or after the missing year (2003) of data \n# -&gt; select only the now wanted variables\n# --&gt; had to ungroup in prior step so that year and month aren't automatically included in the select statement by dplyr\n(data_monlthy_avg &lt;- data_climate %&gt;% \n  mutate(year = year(Date),\n         month = month(Date)) %&gt;% \n  group_by(Location,\n           year,\n           month,\n           Element) %&gt;% \n  summarize(day = max(day(Date)),\n            value_avg = round(mean(value), 1)) %&gt;% \n  ungroup %&gt;% \n  mutate(date = ymd(paste(year, month, day, sep = \"-\")),\n         line_plot_group = ifelse(test = year &lt; 2003,\n                                  yes = 1,\n                                  no = 2)) %&gt;% \n  select(Location,\n         date,\n         Element,\n         value_avg,\n         line_plot_group))\n\n# A tibble: 4,896 × 5\n   Location date       Element value_avg line_plot_group\n   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n 1 AK       1980-01-31 PRCP         10.2               1\n 2 AK       1980-01-31 TMAX         -6.4               1\n 3 AK       1980-01-31 TMIN        -14.1               1\n 4 AK       1980-02-29 PRCP         13.4               1\n 5 AK       1980-02-29 TMAX          1.8               1\n 6 AK       1980-02-29 TMIN         -5.2               1\n 7 AK       1980-03-31 PRCP          3.1               1\n 8 AK       1980-03-31 TMAX          1.6               1\n 9 AK       1980-03-31 TMIN         -5.6               1\n10 AK       1980-04-30 PRCP          7.8               1\n# ℹ 4,886 more rows\n\n\n\n1.4.2 Projections\nNow we are going to create the dataset to model the different types of projections that will be included in the final plot. This requires several intermediate datasets before they can be combined into a final projections dataset. Below is the first one, which will contain information about the intitial average values to be used for comparison purposes.\n\nCode# create a dataset that will be used to plot a horizontal line\n# this is an intermediate dataset for the final plotting data\n# the y value should equal the average value during first five years of data collection\n# and the x values will be the first and last dates of data collection so that the line spans the entire time frame\n# -&gt; group by element and location (so the next calculations are group-wise)\n# -&gt; calculate year, the minimum year (in order to get the limit year for the 5 year avg) and the exact first and last dates of data collection\n# -&gt; filter data to only observations in the first 5 years of data collection\n# -&gt; group by again, we need to group by first and last date as well in order to keep those columns (it doesn't affect the following summarizing calculations)\n# -&gt; calculate the average temps for each location\n(data_baseline &lt;- data_climate %&gt;% \n  group_by(Element, \n           Location) %&gt;% \n  mutate(year = year(Date),\n         year_first = min(year),\n         date_first = min(Date),\n         date_last = max(Date)) %&gt;% \n  filter(year &lt; year_first + 5) %&gt;% \n  group_by(Element,\n           Location,\n           date_first,\n           date_last) %&gt;% \n  summarize(value_5avg = round(mean(value), 1)) %&gt;% \n  ungroup)\n\n# A tibble: 9 × 5\n  Element Location date_first date_last  value_5avg\n  &lt;chr&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;\n1 PRCP    AK       1980-01-01 2018-12-31        4.8\n2 PRCP    AU       1971-01-01 2018-12-31        1.7\n3 PRCP    HI       1951-01-01 2002-12-31        4.9\n4 TMAX    AK       1980-01-01 2018-12-31        6  \n5 TMAX    AU       1971-01-01 2018-12-31       19.5\n6 TMAX    HI       1951-01-07 2002-12-31       26.8\n7 TMIN    AK       1980-01-01 2018-12-31       -1.6\n8 TMIN    AU       1971-01-01 2018-12-31        9.4\n9 TMIN    HI       1951-01-02 2002-12-31       18.6\n\n\nNext is the second intermediate dataset, which will capture information based on a smooth curve fitted to the daily data.\n\nCode# goal is to get the predicted value from the last day of data collection based off some model\n# going to do loess regression with response as value (value) and predictor as number of days after (days_since) a round arbitrary start date (01/01/1950) that is before earliest data point \n# this is an intermediate dataset for the final plotting data\n# -&gt; calculate explanatory variable\n# -&gt; split dataset into a list with an element for each unique Location / Element combo\n# -&gt; run lapply for each element that does the following:\n# --&gt; fits a model and all extract predictions\n# --&gt; then create and return a dataframe that contains only the final predicted value and the location / element information\n# -&gt; finally combine each returned one-row dataframes into a single dataframe\n(data_last_pred &lt;- data_climate %&gt;% \n  mutate(days_since = as.numeric(Date - ymd(\"1950-01-01\"))) %&gt;% \n  split(f = paste(.$Location, \n                  .$Element,\n                  sep = \"-\")) %&gt;% \n  mclapply(FUN = function(climate_data) {\n    \n    # fit loess model with specified response and predictor\n    # -&gt; the model fit here matches what will be done with geom_smooth() in the final plot\n    # -&gt; so using the same model parameters, which is span = 2 and is largerish more robust so that the smoothed line is not too fine / jaggedy (want it to be a smooth curve for visual purposes)\n    model = loess(formula = value ~ days_since,\n                  span = 2,\n                  data = climate_data)\n    \n    # extract predicted values across all xs\n    preds = predict(model) %&gt;% \n      round(2)\n    \n    # create an organized one-row organized dataframe to return\n    # -&gt; reduce modeling dataset to just one row that contains that needed id information\n    # -&gt; extract just the last predicted value\n    climate_data %&gt;% \n      distinct(Location,\n               Element) %&gt;% \n      mutate(last_pred = tail(preds, n = 1)) %&gt;% \n      return\n\n  }, mc.cores = 4) %&gt;% \n  Reduce(f = bind_rows, x = .))\n\n# A tibble: 9 × 3\n  Location Element last_pred\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;\n1 AK       PRCP         5.08\n2 AK       TMAX         8.27\n3 AK       TMIN         0.59\n4 AU       PRCP         1.44\n5 AU       TMAX        21.1 \n6 AU       TMIN        10.0 \n7 HI       PRCP         5.25\n8 HI       TMAX        26.1 \n9 HI       TMIN        20.2 \n\n\nNow we can combine both the initial average info and the predictions from the model into the final projections dataset.\n\nCode# create dataset that contains all of the information related to projections, both the simple baseline average and the fancy model\n# this is a final plotting dataset\n# -&gt; add info about the final predicted value to the baseline data\n# now need to create the information about the error bar, which will summarize the differences between the two different projection methods\n# -&gt; do the following calculations:\n# --&gt; difference between the two final predictions\n# --&gt; sign of difference (for the label)\n# --&gt; character string of formatted error label\n# --&gt; location of midpoint of errorbar, which will be the y value of the label position on the final plot\n(data_projections &lt;- data_baseline %&gt;% \n  left_join(data_last_pred, by = c(\"Element\", \"Location\")) %&gt;% \n  ungroup %&gt;% \n  mutate(error = round(last_pred - value_5avg, 2),\n         error_sign = ifelse(error &gt; 0, \"+\", \"-\"),\n         error_label = paste(ifelse(error_sign == \"+\", \"+\", \"\"),\n                       error,\n                       \"˚\",\n                       sep = \"\"),\n         error_midpoint = round((last_pred + value_5avg) / 2, 2)))\n\n# A tibble: 9 × 10\n  Element Location date_first date_last  value_5avg last_pred error error_sign\n  &lt;chr&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     \n1 PRCP    AK       1980-01-01 2018-12-31        4.8      5.08  0.28 +         \n2 PRCP    AU       1971-01-01 2018-12-31        1.7      1.44 -0.26 -         \n3 PRCP    HI       1951-01-01 2002-12-31        4.9      5.25  0.35 +         \n4 TMAX    AK       1980-01-01 2018-12-31        6        8.27  2.27 +         \n5 TMAX    AU       1971-01-01 2018-12-31       19.5     21.1   1.58 +         \n6 TMAX    HI       1951-01-07 2002-12-31       26.8     26.1  -0.71 -         \n7 TMIN    AK       1980-01-01 2018-12-31       -1.6      0.59  2.19 +         \n8 TMIN    AU       1971-01-01 2018-12-31        9.4     10.0   0.63 +         \n9 TMIN    HI       1951-01-02 2002-12-31       18.6     20.2   1.59 +         \n# ℹ 2 more variables: error_label &lt;chr&gt;, error_midpoint &lt;dbl&gt;\n\n\n\n1.4.3 Window setting\nBecause the impact of the final visuals depends heavily on visual comparisons, much care is needed to be taken in terms of the \\(y\\) scales on each plot. Here is the thought process:\n\nWhen facetting with “free_y” scales for each location, visual comparisons of the error bars may be misleading. For accurate visual comparisons of the errors, the total range for each \\(y\\) scale needs to be the same (so the error bars are correctly proportional).\n“free_y” scales with facets will correctly position the limits for each panel’s data, but some data could appear to be zoomed in if the range for \\(y\\) scale is smaller.\nSo we are going to figure out the maximum range between the largest and smallest avg monthly value (for each location). And then use that same range (and approximately centered around each respective baseline) for the other two locations.\nAs a result of the same range effect, the variability in monthly avg values will be presented correctly as well. So this info will be represented as a pair of \\((x,y)\\) coordinates at the desired edges of the values’ scales.\n\nNow lets create a dataset that can reflect the conclusion we arrived at.\n\nCode# create dataset with the information needed for y limits of each panel\n# this is a final plotting dataset \n# -&gt; start with the monthly average values dataset because that data defines the y scale \n# -&gt; group by element and location\n# -&gt; figure out the min, max and range (max - min) values for each\n# now find the max range for each element\n# summarizing over Location because we want to the overall max as the new common max\n# do this by splitting the dataframe by element and an lapply that does the following on each subset:\n# -&gt; figures out the max range\n# -&gt; adds this info to the original subset with a join\n# -&gt; then combine resulting dataframes back into one overall dataframe\n# now we are going to add in the information about the baseline average\n# -&gt; use a join, just want the first date and 5 year avg info from the baseline info\n# -&gt; select and reorder info so can figure out how to set limits\n# -&gt; add width info to baseline dataset\n# manually set bounds for each panels window (just looked at numbers and set window bounds strategically)\n# did this according to the following strategy (for each element):\n# -&gt; window for each panel (location) needs to be the same as the overall max range so that can compare error bars accurately\n# -&gt; and preferably centered around the 5 year average, just for aesthetic purposes\n# -&gt; add this via numeric constants in a mutate statement\n# -&gt; note that rain fall has a natural lower bound of zero, so curves wont really be centered in the windows\n# -&gt; and windows for rain are quite large (relative to values for AK and AU) because of the high variability in HI\n# now want to format data so that it can be handled by ggplot and the facets\n# -&gt; convert to to tall data, creating a column for the value of the bound and which window it corresponds to (lower / upper)\n# -&gt; keep only needed variables\n# --&gt; going to use the the first day of data collection as the x coordinate, it's just an arbitrary choice\n(data_y_limits &lt;- data_monlthy_avg %&gt;% \n  group_by(Element,\n           Location) %&gt;% \n  summarize(across(.cols = value_avg,\n                   .fns = c(\"min\" = min, \"max\" = max),\n                   .names = \"{.fn}\"),\n            range = diff(range(value_avg))) %&gt;% \n  split(f = .$Element) %&gt;% \n  mclapply(FUN = function(avg_data) {\n    \n    # -&gt; calculate the maximum range across locations\n    # -&gt; then add back in original info\n    # -&gt; remove individual location range values\n    avg_data %&gt;% \n      summarize(max_range = max(range)) %&gt;% \n      right_join(avg_data,\n                 by = \"Element\") %&gt;% \n      select(-range) %&gt;% \n      return\n    \n  }, mc.cores = 4) %&gt;% \n  Reduce(f = bind_rows,\n         x = .) %&gt;%  \n  right_join(data_baseline,\n             by = c(\"Element\",\n                    \"Location\")) %&gt;% \n  select(Element,\n         Location,\n         date_first,\n         min,\n         max,\n         max_range,\n         value_5avg) %&gt;% \n  mutate(lower = c(0,0,0,\n                   -13,3.7,10.1,\n                   -20,-5,2.8),\n         upper = c(35,35,35,\n                   20.8,37.5,43.9,\n                   12,25.8,33.6)) %&gt;% \n  pivot_longer(cols = c(lower,\n                        upper),\n               names_to = \"y_window\",\n               values_to = \"bound\") %&gt;% \n  select(Element,\n         Location,\n         date_first, \n         y_window,\n         bound))\n\n# A tibble: 18 × 5\n   Element Location date_first y_window bound\n   &lt;chr&gt;   &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1 PRCP    AK       1980-01-01 lower      0  \n 2 PRCP    AK       1980-01-01 upper     35  \n 3 PRCP    AU       1971-01-01 lower      0  \n 4 PRCP    AU       1971-01-01 upper     35  \n 5 PRCP    HI       1951-01-01 lower      0  \n 6 PRCP    HI       1951-01-01 upper     35  \n 7 TMAX    AK       1980-01-01 lower    -13  \n 8 TMAX    AK       1980-01-01 upper     20.8\n 9 TMAX    AU       1971-01-01 lower      3.7\n10 TMAX    AU       1971-01-01 upper     37.5\n11 TMAX    HI       1951-01-07 lower     10.1\n12 TMAX    HI       1951-01-07 upper     43.9\n13 TMIN    AK       1980-01-01 lower    -20  \n14 TMIN    AK       1980-01-01 upper     12  \n15 TMIN    AU       1971-01-01 lower     -5  \n16 TMIN    AU       1971-01-01 upper     25.8\n17 TMIN    HI       1951-01-02 lower      2.8\n18 TMIN    HI       1951-01-02 upper     33.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#visualizations",
    "href": "visualizations.html#visualizations",
    "title": "1  Visualizations",
    "section": "\n1.5 Visualizations",
    "text": "1.5 Visualizations\n\n1.5.1 Strategy and descriptions\nHere are descriptions of each feature of the final visuals and how they will be constructed.\nOverall plot features / strategy:\n\nNever need to group by location because that gets facetted.\nFiltering dataset for each geom for the correct element so that the code can remain the same between plots for the different responses.\n\n– Only have to set a constant for the element and main color at the start and then the ggplot is set up to incorporate that automatically.\n– Using blue because of Storytelling With Data (SWD).\n\nNeutral colors will be various greys cause not as strong as black.\n\nLine plot of the average monthly values:\n\nPurpose is to show variability over time of avg values.\nNot the focus, so make thinner and slightly transparent.\nAdds the grouping variable so that the gap in the missing data shown with two separate lines rather than connecting across the gap.\n\nSmooth curve of daily temps:\n\nPlots loess model of daily values over time.\nThis model (including its parameters) is the same that was used to get the last predicted value in the projection data.\nThis is a main part of the plot, so thicker line.\n\nBaseline plot:\n\nUses projection data.\nJust a horizontal line at the first 5 year avg of daily values.\nThis is for comparison purposes, not a main focus so choosing a neutral color with a thinner line than the smooth curve (but thicker than the monthly avgs).\n\nError bar:\n\nUses projection data.\nSets \\(x\\) last day of data collection, \\(y\\) values are the last predicted value and baseline avg.\nThis is a standout feature of the plot, color will set manually based on what should be the focus.\nWant to highlight the increases in temp (based on model compared to historical average) for narrative of the entire data viz.\nAnd just changes in precipitation, not focusing on whether the change is an increase or decrease.\n\nText labels for error bars:\n\nUses projection data.\nSet to the right of the error bars and centered at the midpoint.\nColor coordinated with the error bars.\n\nColor scale for error bars and labels:\n\nFor temps set a bright color for the errors that I want to highlight and a neutral color for the rest.\nUsing orange cause of SWD (blue / orange are good contrasting colors that don’t have an immediate connotation like red / green).\nFor precipitation, not highlighting whether a change was \\(+/-\\), so use two bright colors (rather than one bright and one neutral).\n\nWindow (range) setting:\n\nThis plot feature will be set up as an option within the function via a conditional statement.\nSet up as an option so that it there is a concise way to add it in while also easily being able to have or not have it.\nHad to manually look into what to set the window bounds as, so this feature is just for the final visualizations.\nUses plot values limits dataset.\nJust adds data points set at the desired lower and upper bounds for each locations \\(y\\) scale.\nThese data points are just to force the scales to include the correct range, so can make points invisible.\n\nFinal aspects:\n\nFacet by location.\nModify theme slightly.\n\n1.5.2 Function for implementation\nTo implement these, lets define a function that will create a ggplot object with all of the above features that will be common across all plots. This is just for more concise code later; element specific features (such as labels and scale colors) will be added on individually later. And formal titles and captions will be added on last.\n\nCode# define a function that will return a ggplot object with all of the above features\n# -&gt; only need to specify character strings for the element of interest and the main color of the line plots\n# --&gt; default color value is black (which is neutralish) just so that can run code if don't want to specify one\n# this will contain only plot feature that are common across all elements\n# element specific features (such as labels and scale colors) will be tacked on afterwards, as well as the formal titles / captions\n# so when plotting a specific element, just have to call this function and then the rest of the layers that are modified to have info specific to the element of interest\n# note that this function depends on objects in the global environment\n# -&gt; so not necessarily the best way to code it\n# -&gt; but the datasets it requires have to be setup in a specific way according to the above data manipulations\n# -&gt; so this is just much easier and makes sense for how this application was carried out \nfun_common_plot &lt;- function(element,\n                            color = \"black\",\n                            window_setting = FALSE) {\n  \n  # create and save base plot object\n  g = ggplot() +\n    geom_line(aes(x = date,\n                  y = value_avg,\n                  group = line_plot_group),\n              color = color,\n              linewidth = 0.25,\n              alpha = 0.25,\n              data = filter(data_monlthy_avg,\n                            Element == element)) + \n    stat_smooth(aes(x = Date,\n                    y = value),\n                method = \"loess\",\n                span = 2,\n                se = FALSE,\n                color = color,\n                data = filter(data_climate,\n                              Element == element)) + \n    geom_segment(aes(x = date_first,\n                     xend = date_last,\n                     y = value_5avg,\n                     yend = value_5avg),\n                 color = \"grey50\",\n                 data = filter(data_projections,\n                               Element == element)) + \n    geom_errorbar(aes(x = date_last,\n                      ymin = value_5avg,\n                      ymax = last_pred,\n                      color = error_sign),\n                  width = 500,\n                  linewidth = 1, \n                  show.legend = FALSE,\n                  data = filter(data_projections, \n                                Element == element)) + \n    geom_text(aes(x = date_last,\n                  y = error_midpoint,\n                  label = error_label,\n                  color = error_sign),\n              nudge_x = 1500,\n              size = 5,\n              show.legend = FALSE,\n              data = filter(data_projections,\n                            Element == element)) + \n    facet_grid(Location ~ .,\n               scales = \"free_y\") + \n    theme(axis.title.x = element_blank(),\n          panel.border = element_rect(color = \"lightgrey\",\n                                      fill = NA))\n  \n  # conditionally add range setting feature\n  if (window_setting) {\n    g = g + \n      geom_point(aes(x = date_first,\n                     y = bound),\n                 alpha = 0,\n                 data = filter(data_y_limits,\n                               Element == element))\n  }\n  \n  # return plot object\n  return(g)\n\n}\n\n\n\n1.5.3 Minumum temperatures\nCreate base plot for minimum temperatures. This lets the facetting control the \\(y\\) scales and does not activate the window setting feature.\n\nCode# visualize TMIN\n# -&gt; first visualize base plot \nfun_common_plot(element = \"TMIN\",\n                color = \"dodgerblue1\",\n                window_setting = FALSE)\n\n\n\n\n\n\n\n\n\nNow incorporate the specified windows into each panel.\n\nCode# -&gt; now add window setting feature\nfun_common_plot(element = \"TMIN\",\n                color = \"dodgerblue1\",\n                window_setting = TRUE)\n\n\n\n\n\n\n\n\n\nWe see that this addition has the intended affect of setting equally wide and centered windows for each panels \\(y\\) axis, which improves the image and allows for accurate visual comparisons of the error bars. So lets keep it! Now we can create the final visualization for minimum temperatures.\n\nCode# -&gt; then add the rest of the layers with formal info\n(plot_tmin &lt;- fun_common_plot(element = \"TMIN\",\n                             color = \"dodgerblue1\",\n                             window_setting = TRUE) +\n  scale_color_manual(values = c(\"-\" = \"grey60\", \"+\" = \"orange\")) + \n  labs(title = \"Average minimum temperatures over time \\ncompared to average during first five years\",\n      subtitle = \"(with monthly averages overlaid)\",\n      caption = \"Data source: Daily Global Historical Climatology Network (GHCN) \\nPlot by Colton Gearhart\",\n      y = \"Temperature (˚C)\")) \n\n\n\n\n\n\n\n\n\n\n1.5.4 Maximum temperatures\nNow we can do the same process for maximumum temperatures.\n\nCode# visualize TMAX\n# -&gt; first create base plot \nfun_common_plot(element = \"TMAX\",\n                color = \"dodgerblue4\", \n                window_setting = FALSE)\n\n\n\n\n\n\n\n\n\n\nCode# this improves image again, so keep it!\n# -&gt; now add window setting feature\nfun_common_plot(element = \"TMAX\",\n                color = \"dodgerblue4\",\n                window_setting = TRUE)\n\n\n\n\n\n\n\n\n\n\nCode# -&gt; then add the rest of the layers with formal info\n# this is the final visualization for maximum temperatures\n(plot_tmax &lt;- fun_common_plot(element = \"TMAX\",\n                              color = \"dodgerblue4\",\n                              window_setting = TRUE) + \n  scale_color_manual(values = c(\"-\" = \"grey60\", \"+\" = \"orange\")) + \n  labs(title = \"Average maximum temperatures over time \\ncompared to average during first five years\",\n      subtitle = \"(with monthly averages overlaid)\",\n      caption = \"Data source: Daily Global Historical Climatology Network (GHCN) \\nPlot by Colton Gearhart\",\n      y = \"Temperature (˚C)\")) \n\n\n\n\n\n\n\n\n\n\n1.5.5 Precipitation levels\nAnd again for precipitation levels.\n\nCode# visualize PRCP\n#  -&gt; first create base plot \nfun_common_plot(element = \"PRCP\",\n                color = \"turquoise\",\n                window_setting = FALSE)\n\n\n\n\n\n\n\n\n\n\nCode# -&gt; now add window setting feature\nfun_common_plot(element = \"PRCP\",\n                color = \"turquoise\",\n                window_setting = TRUE)\n\n\n\n\n\n\n\n\n\n\nCode# -&gt; then add the rest of the layers with formal info\n(plot_prcp &lt;- fun_common_plot(element = \"PRCP\",\n                             color = \"turquoise\",\n                             window_setting = TRUE) +  \n  scale_color_manual(values = c(\"-\" = \"gold2\", \"+\" = \"plum2\")) + \n  labs(title = \"Average precipitation levels over time \\ncompared to average during first five years\",\n      subtitle = \"(with monthly averages overlaid)\",\n      caption = \"Data source: Daily Global Historical Climatology Network (GHCN) \\nPlot by Colton Gearhart\",\n      y = \"Precipitation (mm)\"))\n\n\n\n\n\n\n\n\n\nNote that there is a natural lower bound of zero for precipitation levels and there is high variability in HI. So by forcing each panel to have the same range via the window setting, effects (i.e. error bars) are barely visible, but there is nothing that can really be done about this given the current implementation strategy. But perhaps the lack of effect (or minimal effect) tells something as well? This would likely be a question for a content expert.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "visualizations.html#save-plots",
    "href": "visualizations.html#save-plots",
    "title": "1  Visualizations",
    "section": "\n1.6 Save plots",
    "text": "1.6 Save plots\nBelow is code that can be used to save ggplots.\n\nCode# save plots\nggsave(filename = \"Files/Images/Visual-TMIN.png\",\n       plot = plot_tmin)\nggsave(filename = \"Files/Images/Visual-TMAX.png\",\n       plot = plot_tmax)\nggsave(filename = \"Files/Images/Visual-PRCP.png\",\n       plot = plot_prcp)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  },
  {
    "objectID": "report.html#climate-change-fact-or-fallacy",
    "href": "report.html#climate-change-fact-or-fallacy",
    "title": "2  Report",
    "section": "Climate Change: Fact or Fallacy?",
    "text": "Climate Change: Fact or Fallacy?\nFor almost a decade now, climate change has been one of the most contentious issues. Claims of “global warming” are cited on a daily basis (National Geographic, n.d.), but are these substantiated? Or is climate change just being overstated? To assess the validity of our concerns, we analyzed data from the National Climatic Data Center (NOAA) through the Global Historical Climatology Network (GHCN) database (Menne et al. 2012). It contains data collected daily from weather stations all over the world from as far back as 1760. To get a sense of how the climate is behaving in different regions of the world, we only analyzed data from three stations in Alaska, Australia and Hawaii. This gave us data from locations with varying latitudes.\nWe first visualized minimum temperatures (°C) over the past several decades. LOESS curves were fit for the daily data and compared to the average minimum temperature during the first five years of data collection. In addition, monthly averages were overlaid to get a sense of variation.\n\n\n\n\n\n\n\nIn all three locations, we can see an upward trend indicating that average minimum temperatures have been increasing. The more drastic changes can be seen in Alaska and Hawaii, where the current averages are 4.58 °C and 1.59 °C above the baseline for the first five years, respectively. Additionally, monthly averages for Hawaii seem to shift upward as the years increase. These trends seem to give evidence to climate change.\nNext, we looked at maximum temperatures over time. This visualization has the same setup as the previous figure. We can see that Alaska and Australia are experiences upward trends again, while Hawaii seems to increase until about 1970 and then begins to decrease.\n\n\n\n\n\n\n\nSo, what do these changes mean? Does a 3.5 °C increase carry any scientific importance? Let’s put this into perspective. According to Peter deMenocal, a paleoclimate scientist at Columbia University in New York, approximately 5 °C separates the modern world from the last ice age, which ended about 15,000 years ago (Geggel, n.d.). Even though this 5 °C is referring to the average global temperature, the localized increases that we have seen in such a short amount of time are a definite cause for concern.\nFinally, we visualized precipitation levels (mm). Again, this figure has the same setup as the previous figures.\n\n\n\n\n\n\n\nThe average precipitation levels seem to be fairly constant and not far off of the average during the first five years for all three locations. However, one thing to keep in mind is that the effects of climate change are regionalized; some areas may experience harsher effects than others. Also, the selected locations are all close to the coast, and the ocean acts like a buffer to moderate the climate.\nTo wrap up, the visualizations presented here have given evidence of a very concerning phenomenon. Climate change will likely remain a hot button issue, but hopefully we can recognize the importance of it and work together to keep a healthy Earth!\n\n\n\n\n\n\nGeggel, Laura. n.d. “How Would Just 2 Degrees of Warming Change the Planet?” https://www.livescience.com/58891-why-2-degrees-celsius-increase-matters.html.\n\n\nMenne, Matthew J., Imke Durre, Bryant Korzeniewski, Shelley McNeill, Kristy Thomas, Xungang Yin, Steven Anthony, et al. 2012. “Global Historical Climatology Network - Daily (GHCN-Daily), Version 3.” NOAA National Centers for Environmental Information. https://doi.org/10.7289/V5D21VHZ.\n\n\nNational Geographic. n.d. “Causes and Effects of Climate Change.” https://www.nationalgeographic.com/environment/article/global-warming-effects.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Report</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Geggel, Laura. n.d. “How Would Just 2 Degrees of Warming Change\nthe Planet?” https://www.livescience.com/58891-why-2-degrees-celsius-increase-matters.html.\n\n\nMenne, Matthew J., Imke Durre, Bryant Korzeniewski, Shelley McNeill,\nKristy Thomas, Xungang Yin, Steven Anthony, et al. 2012. “Global\nHistorical Climatology Network - Daily (GHCN-Daily), Version 3.”\nNOAA National Centers for Environmental Information. https://doi.org/10.7289/V5D21VHZ.\n\n\nNational Geographic. n.d. “Causes and Effects of Climate\nChange.” https://www.nationalgeographic.com/environment/article/global-warming-effects.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "visualizations.html#r-setup",
    "href": "visualizations.html#r-setup",
    "title": "1  Visualizations",
    "section": "\n1.1 R setup",
    "text": "1.1 R setup\n\nCode# load packages\nlibrary(tidyverse)\nlibrary(magrittr)\nlibrary(lubridate)\nlibrary(parallel) # for mclappy()\n\n# create default storytelling with data theme\ntheme_swd &lt;- theme_minimal() + \n  theme(\n    # titles and captions\n    plot.title = element_text(size = rel(1.75),\n                              color = \"grey30\"),\n    plot.subtitle = element_text(size = rel(1.25),\n                                 color = \"grey30\"),\n    plot.caption = element_text(hjust = 0,\n                                color = \"grey30\"),\n    # axes\n    axis.title.x = element_text(hjust = 0,\n                                color = \"grey30\"),\n    axis.title.y = element_text(hjust = 1,\n                                color = \"grey30\"),\n    axis.line = element_line(color = \"grey90\"),\n    axis.ticks = element_line(color = \"grey90\"),\n    # plot background and gridlines\n    panel.background = element_blank(),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank(),\n    # legend\n    legend.title = element_text(color = \"grey30\"),\n    legend.text = element_text(color = \"grey30\")\n  )\n\n# set global plot theme\ntheme_set(theme_swd)\n\n# disable scientific notation for readability purposes\noptions(scipen = 999)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Visualizations</span>"
    ]
  }
]